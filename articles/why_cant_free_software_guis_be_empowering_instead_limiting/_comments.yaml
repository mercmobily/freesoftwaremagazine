-
    name: BenedictArf
    mail: ''
    subject: 'May I recommend some background reading'
    hostname: 90.194.136.147
    created: '1202672820'
    body: "HCI is an area that I'm very interested. I've read a few books and essays that other may find useful in understanding the problems and how to approach them.\r\n\r\n[http://worrydream.com/MagicInk/](http://worrydream.com/MagicInk/)\r\nThis article discusses many things of interested to HCI design but the area that stuck with me is the analysis of the forms in which the HCI interaction occurs. It then proposes how to approach the different types of interaction.\r\n\r\n[The Design of Everyday Things, D Norman](http://www.amazon.com/Design-Everyday-Things-Donald-Norman/dp/0465067107/ref=pd_bbs_sr_1?ie=UTF8&s=books&qid=1202672060&sr=8-1)\r\nThis book discuss how people interact with objects. It doesn't deal with HCI specifically but all the points made are relevant to good interface design. This book really has changed the way I look at everything. For example, I now know why the handle to raise and lower my car window is annoying.\r\n\r\n[The Humane Interface, J Raskin](http://www.amazon.com/Humane-Interface-Directions-Designing-Interactive/dp/0201379376/ref=pd_bbs_sr_1?ie=UTF8&s=books&qid=1202672478&sr=1-1)\r\nThis book is incredible. It first explains the physiological background to HCI and then explains the concepts for good interface design. The last section of the book explains ZUIs (zooming user interfaces). Personally I love the idea of ZUIs and would like to see some working examples. However, I fear that they are fatally flawed as I can't see how they can support multitasking without undermining the whole premise of the ZUI.\r\n\r\nCheers\r\nBenedict  \r\n_2008-02-21 edited by FSM to tidy and liven links_"
-
    name: Beatbug
    mail: ''
    subject: 'Visual interfaces'
    hostname: 71.232.14.107
    created: '1202813927'
    body: "The idea of using graphic blocks to represent discrete sections of code in a stream/flow interface is quite well established and already in place...for musicians working with synthesizers.  A Swedish friend of mine has been using a simple-looking synthesizer hooked up to his Dell and Apple laptops in order to construct instruments entirely within a standard GUI interface.  Different visual blocks are assembled on a field and attached with wires -- output to input -- and each block's function is tweaked with sliders to represent the desired effect on the audio signal.  These definitions can be plugged into one another in any manner Magnus desires.  Once programmed, the synthesizer can be entirely disconnected from the laptops and carried to a gig, ready to go.\r\n\r\nWe absolutely should already have this sort of interface available to us.  How hard would it be to integrate something like this with KDevelop, perhaps as an alternative view to the standard code?  I would find it useful to be able to have the code available alongside a graphical representation of it, especially for object-oriented programming.  A class that was broken (say, missing a closing }) could be indicated by rendering the associated graphic object in red; or the graphic view could be manipulated to verify that a certain set of object methods are private or public.\r\n\r\nA good read!  Thanks!"
-
    name: 'Purple Hexagon'
    mail: ''
    subject: 'eternal debate'
    hostname: 80.7.100.176
    created: '1202855656'
    body: "Your wire programming reminds me of Scratch (http://scratch.mit.edu/).\r\n\r\nYou say <cite>\"With a well-designed GUI, you don’t have to memorize a whole micro-language of commands and options to get things done\"</cite> but it simply isn't true.\r\n\r\nWe don't carry around everything we want to communicate printed on little cards using instantly recognisable glyphs. Because we would need so many we build them from words. Where we use images is because they convey information well. Simple truth proven over thousands of years of recorded history is words versus pictures has no clear winner.\r\n\r\nWe don't draw commands to convey complexity to the computer because we need precision. But we do use a graphic label that captures the whole operation as a label.\r\n\r\nBut reality is memorising the micro-language to build more complex actions from is much more powerful than dragging around lots of little pictures to link up. Again going back to drawing the representation to the computer we can with auto-completion and typing link commands and achieve much higher flexible code density than a dragged box. Any dragged box can be represented by typing in a function name or dragging one in from the same palette you'd use for a diagram.\r\n\r\nThe final factor is level of experience. Yes the GUI rules for my mum. She uses the machine less frequently, the time spent learning runes is wasted because she doesn't use it often enough to make it stick. Plus the actions she wants are few and limited meaning you can catch those few and attach convenient labels. But she doesn't draw pictures of her day to send me, she types because she can convey her message more accurately and quickly that way.\r\n"
-
    name: 'Ryan Cartwright'
    mail: ''
    subject: 'A combination of the two does seem best'
    hostname: 212.159.106.192
    created: '1202859485'
    body: "One of the downsides--for me anyway--of GUIs is the lack of auto-completion and the inability to use them entirely from the keyboard. I find it a lot easier to type two characters and hit tab than scan through a list of commands on a menu. Also I find it frustrating to get so far with GUI keyboard control only to have use the mouse to make the final selection or option.\r\n\r\nAll is not lost though and a proper combination is possible--just perhaps not for an entire window manager. A CAD software firm I once worked for had a dual interface. the main drawing window had a point and click menu of cascading and hierarchical items. The same commands could be reached by typing in a shell window which always remained behind but two or three rows below the graphic window.\r\n\r\nAs you typed the menu would follow you, similarly as you clicked the commands appeared in the shell window. E.g. the command to put a line on the screen was \"create line\" often aliased to \"crl\". If you typed \"crl\", the create menu and the line sub-menu both appeared with a host of options. Most users would type \"crl\" and use trhe mouse to place the end-points on the drawing (co-ordinates were displayed on screen). It was not unknown for users to spend the whole time with one hand on the keyboard and the other on the mouse. It made for very speedy working and helped users learn the syntax very quickly."
-
    name: 'Terry Hancock'
    mail: ''
    subject: 'Convergence is the point here, not competition'
    hostname: 68.93.224.4
    created: '1202876540'
    body: "A set of \"little cards using instantly recognisable glyphs\" is pretty much what a character set is, isn't it? _Writing_ has certainly been a successful alternative to speech for thousands of years. And ultimately, writing derives from iconic representations.\r\n\r\nAnd just as there are many concepts which are difficult to communicate without language, there are an enormous number which are easier to depict with graphics (ask any engineer about that one!).\r\n\r\nAnother piece of literature to consider when thinking about this:\r\n[Understanding Comics](http://www.scottmccloud.com/store/books/uc.html) by Scott McCloud (it's a print book, I don't think there's an online version).\r\n\r\n\"GUI\" has come to mean not just a \"graphical interface\", but a very specific and very limited graphical interface. \"CLI\" has a similarly narrow definition. My point is that the distinction is artificial and there's plenty of room to explore in between these extremes.\r\n\r\nAs for the benefit of learning verbal or textual languages to communicate and why we do that with natural languages instead of, say, drawing pictures to communicate (let's not forget that some people do just that!), the reasons are complex.\r\n\r\nOne point is frequency of use. For a professional programmer, learning a programming language is an obvious requirement, and not onerous, because if you use it everyday, you don't have to worry about forgetting it. Natural language is like this.\r\n\r\nBut there are many applications which are infrequently used (e.g. how many times a week do you set up a printer? Or a web server?). For these applications, having to learn a specialized language is extremely limiting.\r\n\r\nSome initiatives, like standardizing on XML syntax, are useful in this regard, because they reduce the chore of learning new language rules. You are essentially in the position of having to learn new vocabulary, but not new grammar, which is a big step in the right direction. But even XML can be really obtuse.\r\n\r\nOf course, the rules are complex. I think that one of the attractions of Python for engineers and scientists (as opposed to professional programmers) is that, because most of the commands and operators are _words_ instead of _symbols_ (unlike Perl, for example), it's easier to _look up_ code that you don't understand when you see it. That certainly is an interesting contrast with the general theory that graphic/symbolic modes are more \"intuitive\" and therefore easier for infrequent users."
-
    name: 'Terry Hancock'
    mail: ''
    subject: 'Thanks for the links!'
    hostname: 68.93.224.4
    created: '1202878385'
    body: "By the way, since your reference is a book, here's the Wikipedia entry on [Zooming User Interfaces](http://en.wikipedia.org/wiki/Zooming_User_Interface).\r\n\r\nI observe that Blender implements something like this internally, becasue every window, even the buttons windows are zoomable, using the mouse wheel on scrolling mice (I'm sorry, I've forgotten what the alternative is, but there is one, it may be pressing the middle mouse button and moving up and down, but I'm not sure).\r\n\r\nThat would absolutely work with virtual desktops, and with OpenGL graphics, it shouldn't be all that hard to do.\r\n\r\nVirtual desktops is one of the features I really like in KDE (I assume Gnome has something similar). I have 20 desktops configured on this computer, and regularly use the desktop selector to move between different tasks that I'm working on in parallel. \r\n\r\nISTM that the ZUI would be a more fluid version of that basic concept.\r\n"
-
    name: andrewg
    mail: ''
    subject: 'Duplicate functionality'
    hostname: 140.203.154.11
    created: '1202919195'
    body: "Terry, you and your commenters have ploughed up an issue that's been bugging me for years. There are many examples of programs that duplicate functionality between CLI and GUI versions, often developed independently. While it is easy to write a GUI wrapper to a CLI program, the reverse is far from trivial. Even when CLI and GUI alternatives exist there is rarely a feature-for-feature correspondence.\r\n\r\nIn a similar vein, attempts have been made to provide event-driven scripting interfaces to GUI software, most notably Applescript. The disadvantage is that Applescript adds yet another inconsistent interface to the developer's workload, and thus is rarely implemented outside of Apple's own software.\r\n\r\nThe example of the CAD program given above illustrates what can be achieved when alternative interfaces are developed hand-in-hand and self-consistently. This is of course more easily done within a monolithic application than across the cacophony of different software installed on a typical desktop system. \r\n\r\nWouldn't it be great if there was a way for application developers to write their code against an abstracted HCI, and then have the Gnomes and KDEs of the world provide a selection of prebuilt but flexible interfaces (CLI, GUI, CAD-like, event-driven) which would each be applicable to any conformant application? This would mean that each GUI action would always have a corresponding, predictable, CLI option or IPC event. This would have the added advantage that new interface systems could be easily grafted onto existing software."
-
    name: inputexpert
    mail: ''
    subject: 'The limiting factor of the gui and cli was a hardware problem.'
    hostname: 64.40.62.38
    created: '1203402068'
    body: "gui’s and cli’s are limiting, because of a hardware problem.\r\n\r\nBecause of the stand-alone keyboard and mouse, the debate between gui vs. cli was a big issue.  The user had to repetitively move their hand from keyboard to mouse. \r\n\r\nWhen the features and performance of the mouse are integrated into the keyboard, the debate no longer exists.  The gui and cli can peacefully coexist.\r\n\r\nI have developed and been using an advanced keyboard for the last three years, which enables me to point, click, type, and scroll instantaneously, all while my fingers are on the home row.  I have total control over the computer screen.  \r\n\r\nHaving solved the hardware problem to the gui vs. cli issue, I am now working on advanced interfaces for a PhD in advanced input, interface, and interaction technology.  \r\n\r\nThe interface of the future I am working on is an interface that is fully customizable by the user using  gui, cli, and a search box equally.   It sits on top of all applications.  It is a master interface/personal interface.   \r\n\r\nWe all work differently on the computer, so an advanced interface should be fully customizable to the user’s preferences and skill level.   \r\n\r\nConclusion\r\n\r\nThe limiting factor of gui cli advances was a hardware problem. \r\n\r\nfrom the “father of the perfect keyboard”\r\n\r\n"
-
    name: 'Terry Hancock'
    mail: ''
    subject: 'Similar to IBM touchpoint?'
    hostname: 68.93.224.4
    created: '1203565605'
    body: "I'm not certain how your \"perfect keyboard\" is designed, but your description sounds a lot like IBM's \"touch point\" on their early \"Thinkpad\" laptops.\r\n\r\nI had one of those, and I loved it. It was much more comfortable than either a mouse or the touch pad.\r\n\r\nI was really sad when they retired the idea to go with the standard \"touch pad\" arrangement on later laptop models (I understand the reason was that they tended to wear out quickly).\r\n\r\n"
-
    name: 'Ryan Cartwright'
    mail: ''
    subject: Touchpoint
    hostname: 82.108.129.98
    created: '1203582655'
    body: "I also love the touchpoint and where possible try to use laptops with one instead of the glidepoint--although they are getting harder to find these days.\r\n\r\nMuch as I love it though I'm not sure it resolves the problem you discuss in this piece. A touchpoint is just another mouse-like device--albeit one which requires less movement of your hands away from the keyboard. A lot of GUIs tend to favour the mouse user (obviously). Some give keyboard shortcuts but the end-result feels more like an afterthought. Mostly this is because things like auto-completion are missing. There are of course some apps which are the other way around--where the GUI feels more like an accommodation of mouse lovers. GVim springs to mind.\r\n\r\nMy experience of the CAD app I mention above (and this was in the late 80s) has left me wanting more of the same ever since. Perhaps some clever sorts could build a WM which has a permanent shell window open and which enables compatible shell commands/GUI apps to be operated using both keyboard and mouse. For example if you start typing a series of piped shell commands, the GUI could give you a visual representation of them and enable you to right click and select further options/view man pages etc.--handy if you can't remember all of the options for the command.\r\n\r\n@inputexpert: I would love to see more about this keyboard of yours - sounds very interesting. Al ittle mean to tempt us with snippets like that and not provide links to further info /images(!) though :o)\r\n\r\ncheers\r\nRyan"
-
    name: 'Terry Hancock'
    mail: ''
    subject: 'Agreed, but it''s still good...'
    hostname: 68.93.224.4
    created: '1203641509'
    body: "\"Much as I love it though I’m not sure it resolves the problem you discuss in this piece.\"\r\n\r\nNo, I don't think so either. I think the problems are more conceptual than physical.\r\n\r\nBut one thing it does do is to make the transition between mouse and keyboard smoother. This should be a big benefit in something like Blender, for example, where you use both a lot. With a conventional setup, this means using your keyboard with your left (or less dextrous) hand while using the mouse with your right (or more dextrous) hand. That can be a little awkward, though. With the touchpoint, though, the transition is smoother, because your fingers never get far from their touch-typing \"home\" position, so you can continue to type two-handed.\r\n\r\nOf course, this remains hypothetical because my TP-380D had nowhere near the CPU and video hardware to run Blender!\r\n\r\nBut I often wished I had that touchpoint keyboard on my desktop system. I'd seriously consider buying one if I could find it (well, and if the drivers would work).\r\n\r\n"
-
    name: mish
    mail: ''
    subject: 'Hotwire shell'
    hostname: 82.21.101.149
    created: '1204479606'
    body: "You may also want to check out the hotwire shell project - they are working on a shell that is built from GUI elements - for example if you do \"ls\" then you can double click on the listing to open the file.\r\n\r\nThere's lots more to it, so go read\r\n\r\nhttp://hotwire-shell.org/\r\n\r\nThey also have a list of related projects\r\n\r\nhttp://code.google.com/p/hotwire-shell/wiki/RelatedProjectsAndIdeas"
